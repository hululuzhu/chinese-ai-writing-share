{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hululuzhu/chinese-ai-writing-share/blob/main/training/transformer_supervised/couplet_Transformer_Source_Code_V1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQAh-D0FCzwV"
      },
      "source": [
        "# Chinese Couplet Transformer model source code. e.g.\n",
        "\n",
        "```\n",
        "上: 欢天喜地度佳节\n",
        "下: 举国迎春贺新年\n",
        "上: 不待鸣钟已汗颜，重来试手竟何艰\n",
        "下: 只缘沧海常风雨，再去翻身只等闲\n",
        "上: 相思俱付三更月\n",
        "下: 寂寞难留一夜风\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ntLVC-_5FaH3"
      },
      "source": [
        "# Connect to google drive and prepare local files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R4tEmatSFNTf"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive') # mount to google drive to save models after training\n",
        "\n",
        "import tensorflow as tf\n",
        "import os\n",
        "os.environ['TF_KERAS'] = '1'\n",
        "!pip install keras-transformer &> /dev/null\n",
        "from keras_transformer import get_model, decode, get_custom_objects\n",
        "\n",
        "import pathlib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbZJpPPdhH21"
      },
      "source": [
        "## TPU setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aXx7Yz25hH22"
      },
      "outputs": [],
      "source": [
        "resolver = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "tf.config.experimental_connect_to_cluster(resolver)\n",
        "tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "# print(\"All devices: \", tf.config.list_logical_devices('TPU'))\n",
        "strategy = tf.distribute.TPUStrategy(resolver)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9kOwaYYfAAG"
      },
      "source": [
        "# Fetch Data and extract"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gjU1zyfWFj4f"
      },
      "outputs": [],
      "source": [
        "working_dir = \"/tmp/working_dir\"\n",
        "!mkdir -p {working_dir}\n",
        "!wget https://github.com/wb14123/couplet-dataset/releases/download/1.0/couplet.tar.gz -P {working_dir}\n",
        "!ls -l {working_dir}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cSq9OXajj1hG"
      },
      "outputs": [],
      "source": [
        "!mkdir -p {working_dir}/couplet_files\n",
        "!tar -xf {working_dir}/couplet.tar.gz -C {working_dir}/couplet_files\n",
        "# !ls -l -R  /tmp/working_dir/couplet_files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ptcOb8M-Kq2B"
      },
      "outputs": [],
      "source": [
        "!head -1 {working_dir}/couplet_files/couplet/train/in.txt {working_dir}/couplet_files/couplet/train/out.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "peOzerlpke4B"
      },
      "source": [
        "## Get vocabs of all chars"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dvLAnInGj74m"
      },
      "outputs": [],
      "source": [
        "COUPLET_PATH = f'{working_dir}/couplet_files/couplet'\n",
        "token_dict = {\n",
        "    '<PAD>': 0,\n",
        "    '<START>': 1,\n",
        "    '<END>': 2,\n",
        "}\n",
        "with open(f\"{COUPLET_PATH}/vocabs\", \"r\") as f:\n",
        "  for x in f:\n",
        "    c = x.strip()[0]\n",
        "    if c not in token_dict:\n",
        "      token_dict[c] = len(token_dict)\n",
        "\n",
        "for t in ['train', 'test']:\n",
        "  for i in ['in', 'out']:\n",
        "    with open(f\"{COUPLET_PATH}/{t}/{i}.txt\", \"r\") as f:\n",
        "      for line in f:\n",
        "        for cs in line.strip().replace(' ', '').replace('\\n', ''):\n",
        "          for c in cs:\n",
        "            if c not in token_dict:\n",
        "              token_dict[c] = len(token_dict)\n",
        "\n",
        "assert 9132 == len(token_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YsfGuIet71lg"
      },
      "outputs": [],
      "source": [
        "with open(os.path.join('/content/gdrive/MyDrive/ML/Models/szhu_public_062021', 'couplet_vocab.pickle'), 'wb') as handle:\n",
        "    pickle.dump(token_dict, handle)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mDttIXl-l88h"
      },
      "outputs": [],
      "source": [
        "rev_token_dict = {v: k for k, v in token_dict.items()}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGHOCOZTLUDK"
      },
      "source": [
        "# Encode data (chars to char-ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C7t-2Ly6LWTB"
      },
      "outputs": [],
      "source": [
        "MAX_SEQ_LEN = 34  # 32 chars plus start/end\n",
        "\n",
        "def clean_input(rawq):\n",
        "  return rawq.strip().replace(' ', '')\n",
        "\n",
        "def encode(rawq, is_decode_output = False, is_2d=False):\n",
        "  output = []\n",
        "  if not is_decode_output:\n",
        "    output.append([1] if is_2d else 1) # start added to encode/decode outputs\n",
        "  # content encoding\n",
        "  string_leng = len(rawq.strip().replace(' ', ''))\n",
        "  for c in rawq.strip().replace(' ', ''):\n",
        "    if c not in token_dict:\n",
        "      token_dict[c] = len(token_dict)\n",
        "    output.append([token_dict[c]] if is_2d else token_dict[c])\n",
        "  output.append([2] if is_2d else 2) # end\n",
        "  for i in range(MAX_SEQ_LEN - len(output)):\n",
        "    output.append([0] if is_2d else 0) # padding to fixed MAX_SEQ_LEN size\n",
        "  return output\n",
        "\n",
        "train_raw = {\"in\": [], \"out\": [], \"pre\": [], \"post\": [], \"decode_in\": []}\n",
        "test_raw = {\"in\": [], \"out\": [], \"pre\": [], \"post\": [], \"decode_in\": []}\n",
        "total_raw = {'train': train_raw, 'test': test_raw}\n",
        "\n",
        "for t in ['train', 'test']:\n",
        "  for i in ['in', 'out']:\n",
        "    with open(f\"{COUPLET_PATH}/{t}/{i}.txt\", \"r\") as f:\n",
        "      for line in f:\n",
        "        if i == 'out':\n",
        "          total_raw[t]['decode_in'].append(encode(line, False, i=='in'))\n",
        "        total_raw[t][i].append(encode(line, i=='out', i=='out'))\n",
        "        total_raw[t][\"pre\" if i == 'in' else 'post'].append(clean_input(line))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qEK47o_BOTkB"
      },
      "outputs": [],
      "source": [
        "def decode_tokens(token_ids):\n",
        "  output = \"\"\n",
        "  for token_id in token_ids:\n",
        "    if token_id > 2:\n",
        "      output += rev_token_dict[token_id]\n",
        "    elif token_id == 0:\n",
        "      break\n",
        "  return output\n",
        "\n",
        "for inq, indecode, outq in zip(total_raw['train']['in'][:3],\n",
        "                     total_raw['train']['decode_in'][:3],\n",
        "                     total_raw['train']['out'][:3]):\n",
        "  print(inq, \"\\n\", indecode, \"\\n\", outq)\n",
        "  print(decode_tokens(inq), decode_tokens(np.asarray(outq).reshape(-1)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h8ynitPfQLsT"
      },
      "outputs": [],
      "source": [
        "dfs = {}\n",
        "\n",
        "for t in ['train', 'test']:\n",
        "  dfs[t] = pd.DataFrame(\n",
        "      list(zip(total_raw[t]['in'], total_raw[t]['out'], total_raw[t]['pre'], total_raw[t]['post'], total_raw[t]['decode_in'])),\n",
        "      columns =['in', 'out', 'pre', 'post', 'decode_in'])\n",
        "  dfs[t]['in_length']  = dfs[t]['in'].str.len()\n",
        "  dfs[t]['out_length']  = dfs[t]['out'].str.len()\n",
        "  dfs[t]['de_in_length']  = dfs[t]['decode_in'].str.len()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "63PXWgKzSXKO"
      },
      "outputs": [],
      "source": [
        "dfs['train'].describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "keFfK9OWuNoM"
      },
      "source": [
        "# Transformer model and training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0465hKlj5vkE"
      },
      "outputs": [],
      "source": [
        "in_np = np.array(dfs['train']['in'].values.tolist())\n",
        "decode_in_np = np.array(dfs['train']['decode_in'].values.tolist())\n",
        "out_np = np.asarray(dfs['train']['out'].values.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R6sMwRFtU-aA"
      },
      "outputs": [],
      "source": [
        "in_np_test = np.array(dfs['test']['in'].values.tolist())\n",
        "decode_in_np_test = np.array(dfs['test']['decode_in'].values.tolist())\n",
        "out_np_test = np.array(dfs['test']['out'].values.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RYAaGohtXcSa"
      },
      "outputs": [],
      "source": [
        "print(in_np.shape, decode_in_np.shape, out_np.shape)\n",
        "print(in_np_test.shape, decode_in_np_test.shape, out_np_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0C4eYwYx_aYl"
      },
      "outputs": [],
      "source": [
        "with strategy.scope():\n",
        "  num_encoders = 4\n",
        "  num_docoders = 4\n",
        "  num_heads = 8\n",
        "  embed_size = 64 * num_docoders\n",
        "  drop_out_rate = 0.1\n",
        "  model = get_model(\n",
        "    token_num=len(token_dict),\n",
        "    embed_dim=embed_size,\n",
        "    encoder_num=num_encoders,\n",
        "    decoder_num=num_docoders,\n",
        "    head_num=num_heads,\n",
        "    hidden_dim=embed_size,\n",
        "    attention_activation='gelu',\n",
        "    feed_forward_activation='gelu',\n",
        "    dropout_rate=drop_out_rate,\n",
        "    embed_weights=np.random.random((len(token_dict), embed_size)),\n",
        "  )\n",
        "  model.compile(\n",
        "      optimizer=tf.keras.optimizers.Adam(),\n",
        "      loss='sparse_categorical_crossentropy',\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ytBCHjck6yCq"
      },
      "outputs": [],
      "source": [
        "epochs = 80\n",
        "batch_size = 256\n",
        "model.fit(\n",
        "  x=[in_np, decode_in_np],\n",
        "  y=out_np,\n",
        "  batch_size=batch_size,\n",
        "  epochs=epochs,\n",
        "  validation_data=([in_np_test, decode_in_np_test], out_np_test),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDeGJzTKhH3C"
      },
      "source": [
        "## save model weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t-q9YMYshH3C"
      },
      "outputs": [],
      "source": [
        "DRIVE_MODEL_DIR = '/content/gdrive/MyDrive/ML/Models/chinese_couplet_v1'\n",
        "!mkdir -p {DRIVE_MODEL_DIR}\n",
        "model.save_weights(DRIVE_MODEL_DIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qk0h37bTovKU"
      },
      "source": [
        "# Inference, see [this colab](https://github.com/hululuzhu/chinese-ai-writing-share/blob/main/RC_01_AI_Writing_Demo_06_2021.ipynb)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "tpu_couplet_writing_0816",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
